{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dimensions (Depth, Height, Width): (2, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def process_floor_plan(image_path, target_size):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Cannot load image: {image_path}\")\n",
    "    \n",
    "    img = cv2.resize(img, target_size)\n",
    "    _, thresh = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    h, w = img.shape\n",
    "    floor_plan = np.zeros((h, w), dtype=int)\n",
    "    \n",
    "    for contour in contours:\n",
    "        cv2.drawContours(floor_plan, [contour], -1, 1, thickness=cv2.FILLED)\n",
    "    \n",
    "    return floor_plan\n",
    "\n",
    "def process_building(floor_plan_paths, target_size):\n",
    "    floors = [process_floor_plan(path, target_size) for path in floor_plan_paths]\n",
    "    building = np.stack(floors, axis=0)\n",
    "    return building\n",
    "\n",
    "# 예제 사용법\n",
    "# floor_plan_paths = [\"floor1.jpg\", \"floor2.jpg\", \"floor3.jpg\"]\n",
    "floor_plan_paths = [\"floor11.png\", \"floor22.png\"]\n",
    "target_size = (50, 50)\n",
    "\n",
    "try:\n",
    "    eval = process_building(floor_plan_paths, target_size)\n",
    "    \n",
    "    # 배열의 높이, 너비, 크기를 확인합니다.\n",
    "    height, width = target_size\n",
    "    depth = len(floor_plan_paths)\n",
    "    \n",
    "    print(f\"Building dimensions (Depth, Height, Width): ({depth}, {height}, {width})\")\n",
    "    \n",
    "    # 배열 전체를 출력합니다.\n",
    "    # for d in range(depth):\n",
    "    #     plt.figure()\n",
    "    #     plt.imshow(eval[d], cmap='gray', interpolation='none')\n",
    "    #     plt.title(f'Floor {d + 1}')\n",
    "    #     plt.axis('off')  # 축을 숨깁니다.\n",
    "    #     plt.show()\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 랜덤한 환경 생성 함수\n",
    "def create_random_environment(shape, num_obstacles):\n",
    "    env = np.zeros(shape)\n",
    "    for _ in range(num_obstacles):\n",
    "        layer = np.random.randint(0, shape[0])\n",
    "        start_row = np.random.randint(0, shape[1] - 10)\n",
    "        start_col = np.random.randint(0, shape[2] - 10)\n",
    "        direction = np.random.choice(['horizontal', 'vertical'])\n",
    "        \n",
    "        if direction == 'horizontal':\n",
    "            env[layer, start_row, start_col:start_col + 10] = 1\n",
    "        else:\n",
    "            env[layer, start_row:start_row + 10, start_col] = 1\n",
    "    \n",
    "    return env\n",
    "\n",
    "# 랜덤한 가상환경 10개 생성\n",
    "num_environments = 10\n",
    "environments = []\n",
    "\n",
    "for _ in range(num_environments):\n",
    "    env_shape = (3, 50, 50)\n",
    "    num_obstacles = np.random.randint(5, 15)  # 장애물의 개수를 랜덤하게 설정\n",
    "    new_environment = create_random_environment(env_shape, num_obstacles)\n",
    "    environments.append(new_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 환경 클래스 정의\n",
    "class GridEnvironment:\n",
    "    def __init__(self, building):\n",
    "        self.building = building\n",
    "        self.dimensions = building.shape\n",
    "        self.start_position = self.get_random_start_position()\n",
    "        self.agent_position = self.start_position\n",
    "        self.exit_position = (0, 0, 0)\n",
    "\n",
    "    def get_random_start_position(self):\n",
    "        while True:\n",
    "            position = (random.randint(0, self.dimensions[0] - 1),\n",
    "                        random.randint(0, self.dimensions[1] - 1),\n",
    "                        random.randint(0, self.dimensions[2] - 1))\n",
    "            if self.building[position] == 0:\n",
    "                return position\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = self.get_random_start_position()\n",
    "        return self.agent_position\n",
    "\n",
    "    def step(self, action):\n",
    "        new_position = list(self.agent_position)\n",
    "        if action == 0:  # 상\n",
    "            new_position[1] -= 1\n",
    "        elif action == 1:  # 하\n",
    "            new_position[1] += 1\n",
    "        elif action == 2:  # 좌\n",
    "            new_position[2] -= 1\n",
    "        elif action == 3:  # 우\n",
    "            new_position[2] += 1\n",
    "        elif action == 4:  # 위층\n",
    "            new_position[0] -= 1\n",
    "        elif action == 5:  # 아래층\n",
    "            new_position[0] += 1\n",
    "\n",
    "        # 범위를 벗어나지 않도록 처리\n",
    "        new_position[0] = np.clip(new_position[0], 0, self.dimensions[0] - 1)\n",
    "        new_position[1] = np.clip(new_position[1], 0, self.dimensions[1] - 1)\n",
    "        new_position[2] = np.clip(new_position[2], 0, self.dimensions[2] - 1)\n",
    "        \n",
    "        new_position = tuple(new_position)\n",
    "\n",
    "        if self.building[new_position] == 1:\n",
    "            reward = -5  # 부정적 보상 감소\n",
    "            done = False\n",
    "        elif new_position == self.exit_position:\n",
    "            reward = 50  # 긍정적 보상 감소\n",
    "            done = True\n",
    "        else:\n",
    "            old_distance = np.linalg.norm(np.array(self.agent_position) - np.array(self.exit_position))\n",
    "            new_distance = np.linalg.norm(np.array(new_position) - np.array(self.exit_position))\n",
    "            distance_change = old_distance - new_distance\n",
    "            reward = -1\n",
    "            if distance_change > 0:\n",
    "                reward += distance_change * 5  # 추가 보상 증가\n",
    "            done = False\n",
    "            self.agent_position = new_position\n",
    "\n",
    "        return new_position, reward, done\n",
    "\n",
    "class PPOPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PPOPolicy, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# PPO 알고리즘 클래스 정의\n",
    "class PPO:\n",
    "    def __init__(self, policy, value, policy_lr=1e-6, value_lr=1e-3, gamma=0.90, clip_ratio=0.4, device='cpu'):\n",
    "        self.policy = policy.to(device)\n",
    "        self.value = value.to(device)\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=policy_lr)\n",
    "        self.value_optimizer = optim.Adam(self.value.parameters(), lr=value_lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        probs = self.policy(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "    def compute_returns(self, rewards, masks):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for reward, mask in zip(reversed(rewards), reversed(masks)):\n",
    "            R = reward + self.gamma * R * mask\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        states = torch.FloatTensor([traj[0] for traj in trajectories]).to(self.device)\n",
    "        actions = torch.LongTensor([traj[1] for traj in trajectories]).unsqueeze(1).to(self.device)\n",
    "        old_log_probs = torch.cat([traj[2] for traj in trajectories]).to(self.device).detach()\n",
    "        rewards = [traj[3] for traj in trajectories]\n",
    "        masks = [traj[4] for traj in trajectories]\n",
    "\n",
    "        returns = self.compute_returns(rewards, masks)\n",
    "        returns = torch.FloatTensor(returns).unsqueeze(1).to(self.device)\n",
    "\n",
    "        values = self.value(states)\n",
    "        advantages = (returns - values).detach()\n",
    "\n",
    "        for _ in range(10):  # 10번 업데이트\n",
    "            log_probs = self.policy(states).gather(1, actions)\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            value_loss = (returns - self.value(states)).pow(2).mean()\n",
    "\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()\n",
    "\n",
    "# CUDA 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# PPO 초기화\n",
    "input_dim = 3  # 에이전트의 위치는 3차원\n",
    "output_dim = 6  # 행동은 6가지 (상, 하, 좌, 우, 위층, 아래층)\n",
    "policy = PPOPolicy(input_dim, output_dim)\n",
    "value = ValueNetwork(input_dim)\n",
    "ppo = PPO(policy, value, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 1/10000 [00:01<5:04:05,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Steps 699, Reward -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 7/10000 [00:09<3:53:17,  1.40s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습\n",
    "num_episodes = 10000\n",
    "max_steps = 700\n",
    "\n",
    "rewards_list = []\n",
    "steps_list = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):  # tqdm을 사용하여 진행률 표시\n",
    "    env = GridEnvironment(random.choice(environments))  # 랜덤한 환경 선택\n",
    "    state = env.reset()\n",
    "    trajectory = []\n",
    "\n",
    "    episode_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        action, log_prob = ppo.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        mask = 0 if done else 1\n",
    "\n",
    "        trajectory.append((state, action, log_prob, reward, mask))\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    ppo.update(trajectory)\n",
    "\n",
    "    rewards_list.append(episode_reward)\n",
    "    steps_list.append(step + 1)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f'Episode {episode}, Steps {step}, Reward {reward}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 학습된 모델 저장\n",
    "policy_path = \"ppo_policy6.pth\"\n",
    "value_path = \"ppo_value6.pth\"\n",
    "\n",
    "torch.save(policy.state_dict(), policy_path)\n",
    "torch.save(value.state_dict(), value_path)\n",
    "\n",
    "print(f\"Models saved: {policy_path}, {value_path}\")\n",
    "\n",
    "# 모델 로드\n",
    "# policy_loaded = PPOPolicy(input_dim, output_dim)\n",
    "# value_loaded = ValueNetwork(input_dim)\n",
    "\n",
    "# policy_loaded.load_state_dict(torch.load(policy_path))\n",
    "# value_loaded.load_state_dict(torch.load(value_path))\n",
    "\n",
    "# print(\"Models loaded successfully.\")\n",
    "\n",
    "# 학습 결과 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_list)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training: Total Reward per Episode')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps_list)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.title('Training: Steps per Episode')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_averages(lst, chunk_size):\n",
    "    return [np.mean(lst[i:i + chunk_size]) for i in range(0, len(lst), chunk_size)]\n",
    "    \n",
    "rewards_avg_list = calculate_averages(rewards_list, 800)\n",
    "steps_avg_list = calculate_averages(steps_list, 800)\n",
    "\n",
    "print(\"Averaged rewards list:\", rewards_avg_list)\n",
    "print(\"Averaged steps list:\", steps_avg_list)\n",
    "\n",
    "# rewards_list를 문자열로 변환하여 파일에 쓰기\n",
    "with open('rewards_avg_list8.txt', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, rewards_avg_list)))\n",
    "\n",
    "# steps_list를 문자열로 변환하여 파일에 쓰기\n",
    "with open('steps_avg_list8.txt', 'w') as q:\n",
    "    q.write('\\n'.join(map(str, steps_avg_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정책 평가 함수\n",
    "def evaluate_policy(envs, policy, num_episodes=1000, max_steps=800):\n",
    "    policy.eval()\n",
    "    total_rewards = []\n",
    "    total_steps = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for episode in range(num_episodes):\n",
    "            env = GridEnvironment(envs)  \n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            for step in range(max_steps):\n",
    "                action, _ = ppo.select_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            total_rewards.append(episode_reward)\n",
    "            total_steps.append(step + 1)\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_steps = np.mean(total_steps)\n",
    "\n",
    "    print(f\"Evaluation over {num_episodes} episodes: Average Reward: {avg_reward}, Average Steps: {avg_steps}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(total_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Evaluation: Total Reward per Episode')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(total_steps)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps')\n",
    "    plt.title('Evaluation: Steps per Episode')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # 행동 패턴 시각화 추가\n",
    "    visualize_behavior(total_rewards, total_steps)\n",
    "\n",
    "def visualize_behavior(total_rewards, total_steps):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.scatter(total_steps, total_rewards)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Behavior Visualization')\n",
    "    plt.show()\n",
    "\n",
    "# 학습된 정책 평가\n",
    "evaluate_policy(eval, policy, num_episodes=10000, max_steps=800)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ppo_policy ppo_value rewards_avg_list3 steps_avg_list3 \n",
    "하이퍼파라미터\n",
    "policy_lr=1e-5, value_lr=1e-5, gamma=0.99, clip_ratio=0.2\n",
    "num_episodes = 10000\n",
    "max_steps = 1000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ppo_policy2 ppo_value2 rewards_avg_list4 steps_avg_list4 \n",
    "하이퍼파라미터\n",
    "policy_lr=1e-6, value_lr=1e-6, gamma=0.90, clip_ratio=0.2\n",
    "num_episodes = 10000\n",
    "max_steps = 1000\n",
    "이게 더 좋네"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ppo_policy3 ppo_value3 rewards_avg_list5 steps_avg_list5 \n",
    "하이퍼파라미터\n",
    "policy_lr=1e-6, value_lr=1e-6, gamma=0.90, clip_ratio=0.3\n",
    "num_episodes = 10000\n",
    "max_steps = 800\n",
    "이게 더 좋네"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ppo_policy4 ppo_value4 rewards_avg_list6 steps_avg_list6 \n",
    "하이퍼파라미터\n",
    "policy_lr=1e-6, value_lr=1e-6, gamma=0.90, clip_ratio=0.4\n",
    "num_episodes = 10000\n",
    "max_steps = 700\n",
    "이게 더 좋네"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ppo_policy5 ppo_value5 rewards_avg_list7 steps_avg_list7 \n",
    "하이퍼파라미터\n",
    "policy_lr=1e-6, value_lr=1e-3, gamma=0.90, clip_ratio=0.4\n",
    "num_episodes = 10000\n",
    "max_steps = 700\n",
    "이게 더 좋네"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ppo_policy6 ppo_value6 rewards_avg_list8 steps_avg_list8 \n",
    "하이퍼파라미터\n",
    "policy_lr=1e-6, value_lr=1e-3, gamma=0.90, clip_ratio=0.4\n",
    "num_episodes = 10000\n",
    "max_steps = 700\n",
    "이게 더 좋네"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rein",
   "language": "python",
   "name": "rein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
