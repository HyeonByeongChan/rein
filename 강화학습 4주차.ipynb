{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "class GridEnvironment:\n",
        "    def __init__(self, grid_size=4):\n",
        "        self.grid_size = grid_size\n",
        "        self.start_position = (0, 0)\n",
        "        self.goal_position = (grid_size-1, grid_size-1)\n",
        "        self.state = self.start_position\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_position\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Action: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        \"\"\"\n",
        "        y, x = self.state\n",
        "        if action == 0:  # Up\n",
        "            y = max(y-1, 0)\n",
        "        elif action == 1:  # Down\n",
        "            y = min(y+1, self.grid_size-1)\n",
        "        elif action == 2:  # Left\n",
        "            x = max(x-1, 0)\n",
        "        elif action == 3:  # Right\n",
        "            x = min(x+1, self.grid_size-1)\n",
        "\n",
        "        self.state = (y, x)\n",
        "\n",
        "        # Check if goal is reached\n",
        "        if self.state == self.goal_position:\n",
        "            return self.state, 1, True  # state, reward, done\n",
        "        else:\n",
        "            return self.state, 0, False  # state, reward, done\n",
        "\n",
        "    def render(self):\n",
        "        grid = [['-' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
        "        y, x = self.state\n",
        "        grid[y][x] = 'A'  # Mark the agent's position\n",
        "        grid[self.goal_position[0]][self.goal_position[1]] = 'G'  # Mark the goal position\n",
        "        for row in grid:\n",
        "            print(' '.join(row))\n",
        "        print()\n",
        "\n",
        "# Example usage\n",
        "env = GridEnvironment(grid_size=4)\n",
        "print(\"Initial State:\")\n",
        "env.render()\n",
        "\n",
        "state, reward, done = env.step(1)  # Move down\n",
        "print(\"After moving down:\")\n",
        "env.render()\n",
        "\n",
        "state, reward, done = env.step(3)  # Move right\n",
        "print(\"After moving right:\")\n",
        "env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y8XElSYKMn-",
        "outputId": "173ee5a4-27a4-48d1-a012-dc67c1c05092"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial State:\n",
            "A - - -\n",
            "- - - -\n",
            "- - - -\n",
            "- - - G\n",
            "\n",
            "After moving down:\n",
            "- - - -\n",
            "A - - -\n",
            "- - - -\n",
            "- - - G\n",
            "\n",
            "After moving right:\n",
            "- - - -\n",
            "- A - -\n",
            "- - - -\n",
            "- - - G\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n"
      ],
      "metadata": {
        "id": "WoCSATPANNjo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 128)  # 상태는 (y, x) 좌표이므로 입력 크기는 2\n",
        "        self.fc2 = nn.Linear(128, 4)  # 출력 크기는 4 (Up, Down, Left, Right)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return torch.softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "qHJa9QFpNPac"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reinforce(env, policy_network, optimizer, episodes=1000, gamma=0.99):\n",
        "    for episode in range(episodes):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = torch.tensor([state], dtype=torch.float)\n",
        "            probs = policy_network(state)\n",
        "            m = Categorical(probs)\n",
        "            action = m.sample()\n",
        "            saved_log_probs.append(m.log_prob(action))\n",
        "\n",
        "            state, reward, done = env.step(action.item())\n",
        "            rewards.append(reward)\n",
        "\n",
        "        R = 0\n",
        "        policy_loss = []\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            policy_loss.append(-saved_log_probs.pop() * R)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print('Episode {}: Loss = {:.4f}'.format(episode, policy_loss.item()))\n"
      ],
      "metadata": {
        "id": "Ld2qbTjLNQrZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridEnvironment(grid_size=4)\n",
        "policy_network = PolicyNetwork()\n",
        "optimizer = optim.Adam(policy_network.parameters(), lr=0.01)\n",
        "\n",
        "reinforce(env, policy_network, optimizer, episodes=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d26b3f33NSM7",
        "outputId": "401b9422-56ee-4b1f-bd80-7b0de3811b1c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Loss = 31.8666\n",
            "Episode 100: Loss = 6.1345\n",
            "Episode 200: Loss = 3.1809\n",
            "Episode 300: Loss = 9.8116\n",
            "Episode 400: Loss = 0.9939\n",
            "Episode 500: Loss = 3.4993\n",
            "Episode 600: Loss = 4.3215\n",
            "Episode 700: Loss = 2.9629\n",
            "Episode 800: Loss = 7.4055\n",
            "Episode 900: Loss = 2.6622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def reinforce_with_baseline(env, policy_network, optimizer, episodes=1000, gamma=0.99):\n",
        "#     for episode in range(episodes):\n",
        "#         saved_log_probs = []\n",
        "#         rewards = []\n",
        "#         state = env.reset()\n",
        "#         done = False\n",
        "\n",
        "#         while not done:\n",
        "#             state = torch.tensor([state], dtype=torch.float)\n",
        "#             probs = policy_network(state)\n",
        "#             m = Categorical(probs)\n",
        "#             action = m.sample()\n",
        "#             saved_log_probs.append(m.log_prob(action))\n",
        "\n",
        "#             state, reward, done = env.step(action.item())\n",
        "#             rewards.append(reward)\n",
        "\n",
        "#         R = 0\n",
        "#         policy_loss = []\n",
        "#         returns = []\n",
        "#         for r in reversed(rewards):\n",
        "#             R = r + gamma * R\n",
        "#             returns.insert(0, R)\n",
        "\n",
        "#         returns = torch.tensor(returns)\n",
        "#         returns = (returns - returns.mean()) / (returns.std() + 1e-9)  # 베이스라인 추가\n",
        "\n",
        "#         for log_prob, R in zip(saved_log_probs, returns):\n",
        "#             policy_loss.append(-log_prob * R)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         policy_loss = torch.cat(policy_loss).sum()\n",
        "#         policy_loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         if episode % 100 == 0:\n",
        "#             print(f'Episode {episode}: Loss = {policy_loss.item()}')\n"
      ],
      "metadata": {
        "id": "QiLFuUgvOWu2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# env = GridEnvironment(grid_size=4)\n",
        "# policy_network = PolicyNetwork()\n",
        "# optimizer = optim.Adam(policy_network.parameters(), lr=0.01)\n",
        "\n",
        "# reinforce_with_baseline(env, policy_network, optimizer, episodes=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoQfb0d3OXF3",
        "outputId": "c809479a-e663-430a-cdc7-9cff2ebdd27f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Loss = 0.470025897026062\n",
            "Episode 100: Loss = 0.12820225954055786\n",
            "Episode 200: Loss = -0.23298974335193634\n",
            "Episode 300: Loss = 0.3718506693840027\n",
            "Episode 400: Loss = -0.07241284102201462\n",
            "Episode 500: Loss = 0.2414531111717224\n",
            "Episode 600: Loss = 2.145578384399414\n",
            "Episode 700: Loss = 0.01262718066573143\n",
            "Episode 800: Loss = -2.2827460765838623\n",
            "Episode 900: Loss = 0.014667188748717308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_policy(env, policy_network):\n",
        "    directions = ['↑', '↓', '←', '→']\n",
        "    for y in range(env.grid_size):\n",
        "        for x in range(env.grid_size):\n",
        "            state = torch.tensor([[y, x]], dtype=torch.float)\n",
        "            with torch.no_grad():\n",
        "                probs = policy_network(state)\n",
        "            best_action = torch.argmax(probs).item()\n",
        "            # 목표 지점에는 G를, 시작 지점에는 S를 표시합니다.\n",
        "            if (y, x) == env.goal_position:\n",
        "                print(' G ', end='')\n",
        "            elif (y, x) == env.start_position:\n",
        "                print(' S ', end='')\n",
        "            else:\n",
        "                print(f' {directions[best_action]} ', end='')\n",
        "        print()\n",
        "    print()\n",
        "\n",
        "# 학습된 정책 시각화\n",
        "visualize_policy(env, policy_network)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ILmIXprOZm-",
        "outputId": "8dfe4161-a3b5-49fd-b57f-8dc2a19409a5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " S  →  ↓  ↓ \n",
            " →  →  ↓  ↓ \n",
            " →  →  →  ↓ \n",
            " →  →  →  G \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTglvsZxPqsg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}